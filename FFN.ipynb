{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "569d7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fcbe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,weights,bias):\n",
    "        \"\"\"\n",
    "        inputs : a vector of inputs \n",
    "        weights : a vector of weights \n",
    "        output : the provided output\n",
    "        \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "    \n",
    "    def activation_function(self, x):\n",
    "        # sigmoid for training\n",
    "        return 1/(1+math.exp(-x))\n",
    "    \n",
    "    def Hard_activation_function(self, x):\n",
    "        # step function for prediction (purely academic)\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def predict(self,inputs):\n",
    "        x = np.dot(self.weights, inputs) + self.bias\n",
    "        return self.activation_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    def __init__(self, layer_id, n_inputs, n_neurons):\n",
    "        self.layer_id = layer_id\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        # Vectorized weights and biases\n",
    "        self.weights = np.random.randn(n_neurons, n_inputs) * 0.1\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "        \n",
    "        # values for backprop\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.inputs = None\n",
    "\n",
    "    def activation(self, z):\n",
    "        # Sigmoid\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        a = self.activation(z)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(self.weights, inputs) + self.biases\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "696e39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_outputs, hidden_layers):\n",
    "        self.structure = [n_inputs] + hidden_layers + [n_outputs]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(1, len(self.structure)):\n",
    "            layer = PerceptronLayer(i-1, self.structure[i-1], self.structure[i])\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, y_true, learning_rate=0.1):\n",
    "        n_layers = len(self.layers)\n",
    "        deltas = [None] * n_layers\n",
    "        \n",
    "        # output layer delta\n",
    "        output_layer = self.layers[-1]\n",
    "        deltas[-1] = (output_layer.a - y_true) * output_layer.activation_derivative(output_layer.z)\n",
    "        \n",
    "        # hidden layers delta\n",
    "        for l in range(n_layers - 2, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "            next_layer = self.layers[l+1]\n",
    "            deltas[l] = np.dot(next_layer.weights.T, deltas[l+1]) * layer.activation_derivative(layer.z)\n",
    "        \n",
    "        # update weights and biases\n",
    "        for l in range(n_layers):\n",
    "            layer = self.layers[l]\n",
    "            a_prev = self.layers[l-1].a if l > 0 else layer.inputs\n",
    "            layer.weights -= learning_rate * np.dot(deltas[l], a_prev.T)\n",
    "            layer.biases -= learning_rate * deltas[l]\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs=1000, lr=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for x, y in zip(X_train, Y_train):\n",
    "                x = np.array(x).reshape(-1,1)\n",
    "                y = np.array(y).reshape(-1,1)\n",
    "                output = self.forward(x)\n",
    "                loss += np.sum((output - y)**2)/2\n",
    "                self.backward(y, learning_rate=lr)\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        x = np.array(inputs).reshape(-1,1)\n",
    "        output = self.forward(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7ad1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5245\n",
      "Epoch 500, Loss: 0.5196\n",
      "Epoch 1000, Loss: 0.5162\n",
      "Epoch 1500, Loss: 0.4045\n",
      "Epoch 2000, Loss: 0.1981\n",
      "Epoch 2500, Loss: 0.0172\n",
      "Epoch 3000, Loss: 0.0074\n",
      "Epoch 3500, Loss: 0.0046\n",
      "Epoch 4000, Loss: 0.0033\n",
      "Epoch 4500, Loss: 0.0025\n"
     ]
    }
   ],
   "source": [
    "net = FeedForwardNeuralNetwork(2, 1, [2])\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [[0],[1],[1],[0]]\n",
    "net.train(X, Y, epochs=5000, lr=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
