{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "569d7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fcbe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,weights,bias):\n",
    "        \"\"\"\n",
    "        inputs : a vector of inputs \n",
    "        weights : a vector of weights \n",
    "        output : the provided output\n",
    "        \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "    \n",
    "    def activation_function(self, x):\n",
    "        # sigmoid for training\n",
    "        return 1/(1+math.exp(-x))\n",
    "    \n",
    "    def Hard_activation_function(self, x):\n",
    "        # step function for prediction (purely academic)\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def predict(self,inputs):\n",
    "        x = np.dot(self.weights, inputs) + self.bias\n",
    "        return self.activation_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9723b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    def __init__(self, layer_id, n_inputs, n_neurons):\n",
    "        self.layer_id = layer_id\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        # Vectorized weights and biases\n",
    "        self.weights = np.random.randn(n_neurons, n_inputs) * 0.1\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "        \n",
    "        # values for backprop\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.inputs = None\n",
    "\n",
    "    def activation(self, z):\n",
    "        # Sigmoid\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        a = self.activation(z)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(self.weights, inputs) + self.biases\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "696e39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_outputs, hidden_layers):\n",
    "        self.structure = [n_inputs] + hidden_layers + [n_outputs]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(1, len(self.structure)):\n",
    "            layer = PerceptronLayer(i-1, self.structure[i-1], self.structure[i])\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self,y_true,eta = 0.1):\n",
    "        n_layers = len(self.layers)\n",
    "        deltas = [None]*n_layers\n",
    "\n",
    "        output_layer = self.layers[-1]\n",
    "        deltas[-1] = (output_layer.a-y_true)*output_layer.activation_derivative(output_layer.z) # this is delta_k for the output layer\n",
    "\n",
    "        for l in range(n_layers-2,-1,-1):\n",
    "            layer=self.layers[l]\n",
    "            next_layer = self.layers[l+1]\n",
    "            deltas[l] = np.dot(next_layer.weights.T,deltas[l+1])*layer.activation_derivative(layer.z)\n",
    "        \n",
    "        for l in range(n_layers):\n",
    "            layer = self.layers[l]\n",
    "            a_prev = self.layers[l-1].a if l>0 else layer.inputs\n",
    "            layer_weights -= eta * np.dot(deltas[l],a_prev.T)\n",
    "            layer_biases -= eta * deltas[l]\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs=1000, lr=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for x, y in zip(X_train, Y_train):\n",
    "                x = np.array(x).reshape(-1,1)\n",
    "                y = np.array(y).reshape(-1,1)\n",
    "                output = self.forward(x)\n",
    "                loss += np.sum((output - y)**2)/2\n",
    "                self.backward(y, eta=lr)\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7ad1a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'layer_weights' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m X = [[\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m],[\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m],[\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m],[\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m]]\n\u001b[32m      3\u001b[39m Y = [[\u001b[32m0\u001b[39m],[\u001b[32m1\u001b[39m],[\u001b[32m1\u001b[39m],[\u001b[32m0\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mFeedForwardNeuralNetwork.train\u001b[39m\u001b[34m(self, X_train, Y_train, epochs, lr)\u001b[39m\n\u001b[32m     40\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.forward(x)\n\u001b[32m     41\u001b[39m     loss += np.sum((output - y)**\u001b[32m2\u001b[39m)/\u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m500\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mFeedForwardNeuralNetwork.backward\u001b[39m\u001b[34m(self, y_true, eta)\u001b[39m\n\u001b[32m     29\u001b[39m layer = \u001b[38;5;28mself\u001b[39m.layers[l]\n\u001b[32m     30\u001b[39m a_prev = \u001b[38;5;28mself\u001b[39m.layers[l-\u001b[32m1\u001b[39m].a \u001b[38;5;28;01mif\u001b[39;00m l>\u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m layer.inputs\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mlayer_weights\u001b[49m -= eta * np.dot(deltas[l],a_prev.T)\n\u001b[32m     32\u001b[39m layer_biases -= eta * deltas[l]\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'layer_weights' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "net = FeedForwardNeuralNetwork(2, 1, [2])\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [[0],[1],[1],[0]]\n",
    "net.train(X, Y, epochs=5000, lr=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
