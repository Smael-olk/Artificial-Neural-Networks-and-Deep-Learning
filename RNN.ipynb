{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569d7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b72b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    def __init__(self, layer_id, n_inputs, n_neurons):\n",
    "        self.layer_id = layer_id\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        # Vectorized weights and biases\n",
    "        self.weights = np.random.randn(n_neurons, n_inputs) * 0.1\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "        \n",
    "        # values for backprop\n",
    "        self.z = None\n",
    "\n",
    "    def activation(self, z):\n",
    "        # Sigmoid\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        a = self.activation(z)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(self.weights, inputs) + self.biases\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer:\n",
    "    def __init__(self, layer_id,n_neurons,n_inputs ):\n",
    "        \"\"\"\n",
    "        layer_id : the id of the layer\n",
    "        n_neurons : number of neurons in the layer\n",
    "        n_recurrent_inputs : number of recurrent inputs\n",
    "        n_inputs : number of inputs\n",
    "        \"\"\"\n",
    "        self.layer_id = layer_id\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_inputs = n_inputs\n",
    "\n",
    "        self.z_history = []\n",
    "        self.h_history = []\n",
    "        self.x_history = []\n",
    "        \n",
    "        self.input_weights = np.random.rand(n_neurons,n_inputs)\n",
    "        self.recurrent_weights = np.random.rand(n_neurons,n_neurons)\n",
    "        self.biases = np.zeros((n_neurons, 1))\n",
    "\n",
    "        self.dW_xh = np.zeros_like(self.input_weights)\n",
    "        self.dW_hh = np.zeros_like(self.recurrent_weights)\n",
    "        self.db = np.zeros_like(self.biases)\n",
    "\n",
    "        self.h_prev = np.zeros((n_neurons, 1))\n",
    "\n",
    "    def activation(self,z):\n",
    "        # Tanh activation function\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def activation_derivative(self,z):\n",
    "        # Derivative of Tanh activation function\n",
    "        h=self.activation(z)\n",
    "        return 1 - h**2\n",
    "        \n",
    "    def forward_step(self, h_prev, x_t):\n",
    "        z_t = np.dot(self.input_weights, x_t) + np.dot(self.recurrent_weights, h_prev) + self.biases\n",
    "        h_t = self.activation(z_t)\n",
    "        return z_t, h_t\n",
    "    \n",
    "    def forward(self, x_sequence):\n",
    "        \"\"\"\n",
    "        x_sequence : input sequence of shape (n_inputs, sequence_length)\n",
    "        \"\"\"\n",
    "        sequence_length = x_sequence.shape[1]\n",
    "        \n",
    "        \n",
    "        h_t = self.h_prev\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_t = x_sequence[:, t].reshape(-1, 1)\n",
    "            z_t, h_t = self.forward_step(self.input_weights, self.recurrent_weights, self.biases, h_t, x_t)\n",
    "            \n",
    "            self.z_history.append(z_t)\n",
    "            self.h_history.append(h_t)\n",
    "            self.x_history.append(x_t)\n",
    "        \n",
    "        self.h_prev = h_t\n",
    "        \n",
    "        return self.h_history, self.z_history\n",
    "    \n",
    "    def backward(self,delta_h_T):\n",
    "        T=len(self.h_history)\n",
    "        self.dW_xh.fill(0)\n",
    "        self.dW_hh.fill(0)\n",
    "        self.db.fill(0)\n",
    "        delta_h_next = np.zeros_like(delta_h_T)\n",
    "\n",
    "        for t in range(T-1,-1,-1):\n",
    "            z_t = self.z_history[t]\n",
    "            h_prev = self\n",
    "            x_t = self.x_history[t]\n",
    "\n",
    "            delta_output = delta_h_T if t == T-1 else np.zeros_like(delta_h_T)\n",
    "            delta_recurrent = np.dot(self.recurrent_weights.T, delta_h_next)\n",
    "            delta_pre_act = delta_output + delta_recurrent\n",
    "\n",
    "            delta_z_t = delta_pre_act * self.activation_derivative(z_t)\n",
    "\n",
    "            self.dW_hh += np.dot(delta_z_t, h_prev.T)\n",
    "            self.dW_xh += np.dot(delta_z_t, x_t.T)\n",
    "            self.db += delta_z_t\n",
    "\n",
    "            delta_h_next = delta_pre_act\n",
    "\n",
    "            return \n",
    "        \n",
    "    def update_weights(self, learning_rate):\n",
    "        self.input_weights -= learning_rate * self.dW_xh\n",
    "        self.recurrent_weights -= learning_rate * self.dW_hh\n",
    "        self.biases -= learning_rate * self.db\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.h_prev = np.zeros((self.n_neurons, 1))\n",
    "        self.z_history = []\n",
    "        self.h_history = []\n",
    "        self.x_history = []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_outputs, hidden_layers,recurrent_output_size):\n",
    "        # Initialize the RNN with given structure\n",
    "        recurrent_layer = RecurrentLayer(layer_id=1, n_neurons=recurrent_output_size, n_inputs=n_inputs)\n",
    "        self.layers = [recurrent_layer]\n",
    "        current_input_size = recurrent_output_size\n",
    "       \n",
    "        # Output layer\n",
    "        output_layer = PerceptronLayer(layer_id=2, n_inputs=current_input_size, n_neurons=n_outputs)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "\n",
    "    def forward_sequence(self, x_sequence):\n",
    "        self.layers[0].reset()\n",
    "        h_history, z_history = self.layers[0].forward(x_sequence)\n",
    "        h_T = h_history[-1]\n",
    "        \n",
    "        a=h_T\n",
    "\n",
    "        ffn_output_layer = self.layers[1]\n",
    "\n",
    "        y_hat = ffn_output_layer.forward(a)\n",
    "        return y_hat\n",
    "        \n",
    "\n",
    "    def backpropagation_through_time(self, x_sequence, target_y, learning_rate):\n",
    "        \n",
    "        y_hat = self.forward_sequence(x_sequence)\n",
    "        # Compute loss and initial delta\n",
    "        delta_y_hat = (y_hat - target_y)**2  # Assuming mean squared error loss\n",
    "        ffn_output_layer = self.layers[1]\n",
    "        delta_h_T = ffn_output_layer.backward(delta_y_hat)\n",
    "        recurrent_layer = self.layers[0]\n",
    "        recurrent_layer.backward(delta_h_T)\n",
    "        recurrent_layer.update_weights(learning_rate)\n",
    "        ffn_output_layer.update_weights(learning_rate)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
